{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# basic libraries for reading data and plotting:\n",
    "#------------------------------\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n",
    "\n",
    "# models,scoring, hyperparameter tuning libraries:\n",
    "# --------------------------------------\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import lightgbm as lgb # Light GBM model\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from random import random\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "#feature importance:\n",
    "#from yellowbrick.model_selection import RFECV\n",
    "\n",
    "#for results table:\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-1 (with LGB models) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(Query_point):\n",
    "    \n",
    "    # reading test_point:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    data_columns = ['channelGrouping', 'date', 'fullVisitorId', 'visitId', 'visitNumber',\n",
    "                    'visitStartTime', 'device.browser', 'device.operatingSystem',\n",
    "                    'device.isMobile', 'device.deviceCategory', 'geoNetwork.continent',\n",
    "                    'geoNetwork.subContinent', 'geoNetwork.country', 'geoNetwork.region',\n",
    "                    'geoNetwork.metro', 'geoNetwork.city', 'geoNetwork.networkDomain',\n",
    "                    'totals.hits', 'totals.pageviews', 'totals.timeOnSite',\n",
    "                    'totals.sessionQualityDim', 'totals.transactions',\n",
    "                    'totals.transactionRevenue', 'trafficSource.referralPath',\n",
    "                    'trafficSource.campaign', 'trafficSource.source',\n",
    "                    'trafficSource.medium', 'trafficSource.keyword',\n",
    "                    'trafficSource.adContent']\n",
    "    \n",
    "    test_data = pd.DataFrame(data = Query_point ,columns=data_columns)\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # pre-processing the test point :\n",
    "    #----------------------------------------\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the boolean feature:\n",
    "    #----------------------------------------\n",
    "    test_data['device.isMobile']  = test_data['device.isMobile'].astype(bool)\n",
    "    print(\"Boolean feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the numerical feature:\n",
    "    #----------------------------------------\n",
    "    numeric_feat = ['visitNumber','visitStartTime','totals.hits','totals.pageviews',\n",
    "                    'totals.timeOnSite','totals.transactions']\n",
    "    \n",
    "    for col in numeric_feat:\n",
    "        test_data[col].fillna(0,inplace=True)\n",
    "        test_data[col] = test_data[col].astype('float')\n",
    "    \n",
    "    print(\"Numerical feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the categorical features:\n",
    "    #----------------------------------------\n",
    "    categorical_feat = ['channelGrouping','device.browser','device.operatingSystem','device.deviceCategory',\n",
    "                        'geoNetwork.continent','geoNetwork.subContinent','geoNetwork.country','geoNetwork.region',\n",
    "                        'geoNetwork.metro','geoNetwork.city','geoNetwork.networkDomain','totals.sessionQualityDim',\n",
    "                        'trafficSource.campaign','trafficSource.source','trafficSource.medium','trafficSource.keyword',\n",
    "                        'trafficSource.referralPath', 'trafficSource.adContent']\n",
    "    \n",
    "    for feature in categorical_feat:\n",
    "    \n",
    "        label_encoder = preprocessing.LabelEncoder() # intitalizing label encoder object\n",
    "    \n",
    "        label_encoder.classes_ = np.load(feature+'.npy') # reading all-ready saved files       \n",
    "        \n",
    "        test_data[feature]  = label_encoder.transform(list(test_data[feature].values.astype('str')))\n",
    "        \n",
    "    \n",
    "    print(\"categorical feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Featurization of query data point:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    test_frame_k_maxdate = max(test_data['date'])\n",
    "    test_frame_k_mindate = min(test_data['date'])\n",
    "    \n",
    "    test_data_featurized = test_data.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork.networkDomain': [('networkDomain' , lambda x: x.dropna().max())], #max value of network domain\n",
    "            'geoNetwork.city':          [('city' , lambda x: x.dropna().max())],  #max value of city\n",
    "            'device.operatingSystem':   [('operatingSystem' , lambda x: x.dropna().max())],  #max value of Operating System\n",
    "            'geoNetwork.metro':         [('metro' , lambda x: x.dropna().max())],  #max value of metro\n",
    "            'geoNetwork.region':        [('region' , lambda x: x.dropna().max())],   #max vaue of region\n",
    "            'channelGrouping':          [('channelGrouping' , lambda x: x.dropna().max())],  #max value of channel grouping\n",
    "          'trafficSource.referralPath': [('referralPath' , lambda x: x.dropna().max())],  #max value of referral path\n",
    "            'geoNetwork.country':       [('country' , lambda x: x.dropna().max())],    #max value of country\n",
    "            'trafficSource.source':     [('source' , lambda x: x.dropna().max())],   #max value of source\n",
    "            'trafficSource.medium':     [('medium' , lambda x: x.dropna().max())],   #max value of medium\n",
    "            'trafficSource.keyword':    [('keyword', lambda x: x.dropna().max())], #max value of keyboard\n",
    "            'device.browser':           [('browser' , lambda x: x.dropna().max())],  #max value of browser\n",
    "            'device.deviceCategory':    [('deviceCategory', lambda x: x.dropna().max())], #max of device category\n",
    "            'geoNetwork.continent':     [('continent' , lambda x: x.dropna().max())],      #max of continent value\n",
    "            'geoNetwork.subContinent':  [('subcontinent' , lambda x: x.dropna().max())],  #max of sub_continent value\n",
    "            'totals.timeOnSite':        [('timeOnSite_sum'  , lambda x: x.dropna().sum()),     # total timeonsite of user\n",
    "                                         ('timeOnSite_min'  , lambda x: x.dropna().min()),     # min timeonsite\n",
    "                                         ('timeOnSite_max'  , lambda x: x.dropna().max()),     # max timeonsite\n",
    "                                         ('timeOnSite_mean' , lambda x: x.dropna().mean())],  # mean timeonsite\n",
    "            'totals.pageviews':         [('pageviews_sum'  , lambda x: x.dropna().sum()),     # total of page views\n",
    "                                         ('pageviews_min'  , lambda x: x.dropna().min()),     # min of page views\n",
    "                                         ('pageviews_max'  , lambda x: x.dropna().max()),     # max of page views\n",
    "                                         ('pageviews_mean' , lambda x: x.dropna().mean())],  # mean of page views\n",
    "            'totals.hits':              [('hits_sum'  , lambda x: x.dropna().sum()),     # total of hits\n",
    "                                         ('hits_min'  , lambda x: x.dropna().min()),     # min of hits\n",
    "                                         ('hits_max'  , lambda x: x.dropna().max()),     # max of hits\n",
    "                                         ('hits_mean' , lambda x: x.dropna().mean())],  # mean of hits\n",
    "            'visitStartTime':           [('visitStartTime_counts' , lambda x: x.dropna().count())], #Count of visitStartTime\n",
    "            'totals.sessionQualityDim': [('sessionQualityDim' , lambda x: x.dropna().max())], #Max value of sessionQualityDim\n",
    "            'device.isMobile':          [('isMobile' ,  lambda x: x.dropna().max())], #Max value of isMobile\n",
    "            'visitNumber':              [('visitNumber_max' , lambda x: x.dropna().max())],  #Maximum number of visits.        \n",
    "            'totals.transactions' :     [('transactions' , lambda x:x.dropna().sum())], #Summation of all the transaction counts.\n",
    "            'date':                     [('first_ses_from_the_period_start' , lambda x: x.dropna().min() - test_frame_k_mindate), #first shopping session for customer after the period end date for current frame.\n",
    "                                         ('last_ses_from_the_period_end', lambda x: test_frame_k_maxdate - x.dropna().max()), #Last shopping session for customer before the period end date for current frame.\n",
    "                                         ('interval_dates' , lambda x: x.dropna().max() - x.dropna().min()),  #interval calculated as the latest date on which customer visited - oldest date on which they visited.\n",
    "                                         ('unqiue_date_num' , lambda x: len(set(x.dropna())))] , # Unique number of dates customer visited.           \n",
    "                                                         })\n",
    "\n",
    "    \n",
    "    # Drop the parent level of features. for e.g. drop geoNetwork.networkDomain and keep only 'networkDomain' which stores max value from the group. \n",
    "    test_data_featurized.columns = test_data_featurized.columns.droplevel() \n",
    "    test_data_featurized         = test_data_featurized.reset_index()\n",
    "    \n",
    "    print(\"feature engineering process done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # passing Query point to models trianed on best best hyperparameter values:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    # Reading pretrained classification model:\n",
    "    #----------------------------------------------\n",
    "    Pkl_Filename = \"lgb_classification_model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        lgb_classification_model = pickle.load(file)\n",
    "    \n",
    "    classification_pred  = lgb_classification_model.predict(test_data_featurized.drop('fullVisitorId', axis=1))     \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Reading pretrained regression model:\n",
    "    #----------------------------------------------\n",
    "    Pkl_Filename = \"lgb_regression_model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        lgb_regression_model = pickle.load(file)\n",
    "        \n",
    "    regression_pred      = lgb_regression_model.predict(test_data_featurized.drop('fullVisitorId', axis=1))\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    \n",
    "        \n",
    "    final_prediction     =  classification_pred*regression_pred\n",
    "    \n",
    "    print(\"prediction for query point done..!\")\n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # returning the model_predictions:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean feature preprocessing done..!\n",
      "Numerical feature preprocessing done..!\n",
      "categorical feature preprocessing done..!\n",
      "feature engineering process done..!\n",
      "prediction for query point done..!\n"
     ]
    }
   ],
   "source": [
    "#reading our test data(unzipped file):\n",
    "#--------------------------------------\n",
    "test_df  = pd.read_csv('case study data/preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},index_col=0).reset_index()\n",
    "#--------------------------------------\n",
    "\n",
    "\n",
    "# passing the first 10 rows of test data as a nd-array for predictions:\n",
    "\n",
    "predictions = final_fun_1(test_df.iloc[0:9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00223092, 0.10557366, 0.0056328 , 0.01775645, 0.04549532,\n",
       "       0.2376245 , 0.01432871, 0.0045344 , 0.08579153])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-2 (using ensemble models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(Query_point):\n",
    "    \n",
    "    # reading test_point:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    data_columns = ['channelGrouping', 'date', 'fullVisitorId', 'visitId', 'visitNumber',\n",
    "                    'visitStartTime', 'device.browser', 'device.operatingSystem',\n",
    "                    'device.isMobile', 'device.deviceCategory', 'geoNetwork.continent',\n",
    "                    'geoNetwork.subContinent', 'geoNetwork.country', 'geoNetwork.region',\n",
    "                    'geoNetwork.metro', 'geoNetwork.city', 'geoNetwork.networkDomain',\n",
    "                    'totals.hits', 'totals.pageviews', 'totals.timeOnSite',\n",
    "                    'totals.sessionQualityDim', 'totals.transactions',\n",
    "                    'totals.transactionRevenue', 'trafficSource.referralPath',\n",
    "                    'trafficSource.campaign', 'trafficSource.source',\n",
    "                    'trafficSource.medium', 'trafficSource.keyword',\n",
    "                    'trafficSource.adContent']\n",
    "    \n",
    "    test_data = pd.DataFrame(data = Query_point ,columns=data_columns)\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # pre-processing the test point :\n",
    "    #----------------------------------------\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the boolean feature:\n",
    "    #----------------------------------------\n",
    "    test_data['device.isMobile']  = test_data['device.isMobile'].astype(bool)\n",
    "    print(\"Boolean feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the numerical feature:\n",
    "    #----------------------------------------\n",
    "    numeric_feat = ['visitNumber','visitStartTime','totals.hits','totals.pageviews',\n",
    "                    'totals.timeOnSite','totals.transactions']\n",
    "    \n",
    "    for col in numeric_feat:\n",
    "        test_data[col].fillna(0,inplace=True)\n",
    "        test_data[col] = test_data[col].astype('float')\n",
    "    \n",
    "    print(\"Numerical feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Handling the categorical features:\n",
    "    #----------------------------------------\n",
    "    categorical_feat = ['channelGrouping','device.browser','device.operatingSystem','device.deviceCategory',\n",
    "                        'geoNetwork.continent','geoNetwork.subContinent','geoNetwork.country','geoNetwork.region',\n",
    "                        'geoNetwork.metro','geoNetwork.city','geoNetwork.networkDomain','totals.sessionQualityDim',\n",
    "                        'trafficSource.campaign','trafficSource.source','trafficSource.medium','trafficSource.keyword',\n",
    "                        'trafficSource.referralPath', 'trafficSource.adContent']\n",
    "    \n",
    "    for feature in categorical_feat:\n",
    "    \n",
    "        label_encoder = preprocessing.LabelEncoder() # intitalizing label encoder object\n",
    "    \n",
    "        label_encoder.classes_ = np.load(feature+'.npy') # reading all-ready saved files       \n",
    "        \n",
    "        test_data[feature]  = label_encoder.transform(list(test_data[feature].values.astype('str')))\n",
    "        \n",
    "    \n",
    "    print(\"categorical feature preprocessing done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Featurization of query data point:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    test_frame_k_maxdate = max(test_data['date'])\n",
    "    test_frame_k_mindate = min(test_data['date'])\n",
    "    \n",
    "    test_data_featurized = test_data.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork.networkDomain': [('networkDomain' , lambda x: x.dropna().max())], #max value of network domain\n",
    "            'geoNetwork.city':          [('city' , lambda x: x.dropna().max())],  #max value of city\n",
    "            'device.operatingSystem':   [('operatingSystem' , lambda x: x.dropna().max())],  #max value of Operating System\n",
    "            'geoNetwork.metro':         [('metro' , lambda x: x.dropna().max())],  #max value of metro\n",
    "            'geoNetwork.region':        [('region' , lambda x: x.dropna().max())],   #max vaue of region\n",
    "            'channelGrouping':          [('channelGrouping' , lambda x: x.dropna().max())],  #max value of channel grouping\n",
    "          'trafficSource.referralPath': [('referralPath' , lambda x: x.dropna().max())],  #max value of referral path\n",
    "            'geoNetwork.country':       [('country' , lambda x: x.dropna().max())],    #max value of country\n",
    "            'trafficSource.source':     [('source' , lambda x: x.dropna().max())],   #max value of source\n",
    "            'trafficSource.medium':     [('medium' , lambda x: x.dropna().max())],   #max value of medium\n",
    "            'trafficSource.keyword':    [('keyword', lambda x: x.dropna().max())], #max value of keyboard\n",
    "            'device.browser':           [('browser' , lambda x: x.dropna().max())],  #max value of browser\n",
    "            'device.deviceCategory':    [('deviceCategory', lambda x: x.dropna().max())], #max of device category\n",
    "            'geoNetwork.continent':     [('continent' , lambda x: x.dropna().max())],      #max of continent value\n",
    "            'geoNetwork.subContinent':  [('subcontinent' , lambda x: x.dropna().max())],  #max of sub_continent value\n",
    "            'totals.timeOnSite':        [('timeOnSite_sum'  , lambda x: x.dropna().sum()),     # total timeonsite of user\n",
    "                                         ('timeOnSite_min'  , lambda x: x.dropna().min()),     # min timeonsite\n",
    "                                         ('timeOnSite_max'  , lambda x: x.dropna().max()),     # max timeonsite\n",
    "                                         ('timeOnSite_mean' , lambda x: x.dropna().mean())],  # mean timeonsite\n",
    "            'totals.pageviews':         [('pageviews_sum'  , lambda x: x.dropna().sum()),     # total of page views\n",
    "                                         ('pageviews_min'  , lambda x: x.dropna().min()),     # min of page views\n",
    "                                         ('pageviews_max'  , lambda x: x.dropna().max()),     # max of page views\n",
    "                                         ('pageviews_mean' , lambda x: x.dropna().mean())],  # mean of page views\n",
    "            'totals.hits':              [('hits_sum'  , lambda x: x.dropna().sum()),     # total of hits\n",
    "                                         ('hits_min'  , lambda x: x.dropna().min()),     # min of hits\n",
    "                                         ('hits_max'  , lambda x: x.dropna().max()),     # max of hits\n",
    "                                         ('hits_mean' , lambda x: x.dropna().mean())],  # mean of hits\n",
    "            'visitStartTime':           [('visitStartTime_counts' , lambda x: x.dropna().count())], #Count of visitStartTime\n",
    "            'totals.sessionQualityDim': [('sessionQualityDim' , lambda x: x.dropna().max())], #Max value of sessionQualityDim\n",
    "            'device.isMobile':          [('isMobile' ,  lambda x: x.dropna().max())], #Max value of isMobile\n",
    "            'visitNumber':              [('visitNumber_max' , lambda x: x.dropna().max())],  #Maximum number of visits.        \n",
    "            'totals.transactions' :     [('transactions' , lambda x:x.dropna().sum())], #Summation of all the transaction counts.\n",
    "            'date':                     [('first_ses_from_the_period_start' , lambda x: x.dropna().min() - test_frame_k_mindate), #first shopping session for customer after the period end date for current frame.\n",
    "                                         ('last_ses_from_the_period_end', lambda x: test_frame_k_maxdate - x.dropna().max()), #Last shopping session for customer before the period end date for current frame.\n",
    "                                         ('interval_dates' , lambda x: x.dropna().max() - x.dropna().min()),  #interval calculated as the latest date on which customer visited - oldest date on which they visited.\n",
    "                                         ('unqiue_date_num' , lambda x: len(set(x.dropna())))] , # Unique number of dates customer visited.           \n",
    "                                                         })\n",
    "\n",
    "    \n",
    "    # Drop the parent level of features. for e.g. drop geoNetwork.networkDomain and keep only 'networkDomain' which stores max value from the group. \n",
    "    test_data_featurized.columns = test_data_featurized.columns.droplevel() \n",
    "    test_data_featurized         = test_data_featurized.reset_index()\n",
    "    \n",
    "    print(\"feature engineering process done..!\")\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # passing Query point to models trianed on best best hyperparameter values:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    # Reading pretrained Ensemble models:\n",
    "    #----------------------------------------------\n",
    "    Pkl_Filename = \"lgb_classification_model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        lgb_classification_model = pickle.load(file)\n",
    "    \n",
    "    classification_pred  = lgb_classification_model.predict(test_data_featurized.drop('fullVisitorId', axis=1))     \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Reading pretrained regression model:\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # 1st ensemble model:\n",
    "    \n",
    "    Pkl_Filename = \"Ensemble_models/Pickle_Rf_Model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        regression_model_1 = pickle.load(file)\n",
    "        \n",
    "    regression_pred_1      = regression_model_1.predict(test_data_featurized.drop('fullVisitorId', axis=1))\n",
    "    \n",
    "    # 2nd ensemble model:\n",
    "    \n",
    "    Pkl_Filename = \"Ensemble_models/Pickle_Lgb_Model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        regression_model_2 = pickle.load(file)\n",
    "        \n",
    "    regression_pred_2      = regression_model_2.predict(test_data_featurized.drop('fullVisitorId', axis=1))\n",
    "    \n",
    "    \n",
    "    # 3rd ensemble model:\n",
    "    \n",
    "    Pkl_Filename = \"Ensemble_models/Pickle_Xgb_Model.pkl\" \n",
    "    \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        regression_model_3 = pickle.load(file)\n",
    "        \n",
    "    regression_pred_3      = regression_model_3.predict(test_data_featurized.drop('fullVisitorId', axis=1))\n",
    "    \n",
    "    \n",
    "    pred_df_reg_1                        = pd.DataFrame({\"fullVisitorId\":test_data_featurized[\"fullVisitorId\"].values})\n",
    "    pred_df_reg_1[\"PredictedLogRevenue\"] = regression_pred_1\n",
    "    pred_df_reg_1.columns                = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "    \n",
    "    pred_df_reg_2                        = pd.DataFrame({\"fullVisitorId\":test_data_featurized[\"fullVisitorId\"].values})\n",
    "    pred_df_reg_2[\"PredictedLogRevenue\"] = regression_pred_2\n",
    "    pred_df_reg_2.columns                = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "    \n",
    "    pred_df_reg_3                        = pd.DataFrame({\"fullVisitorId\":test_data_featurized[\"fullVisitorId\"].values})\n",
    "    pred_df_reg_3[\"PredictedLogRevenue\"] = regression_pred_3\n",
    "    pred_df_reg_3.columns                = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n",
    "    \n",
    "    final_ensemble_pred = pred_df_reg_1.merge(pred_df_reg_2, on='fullVisitorId').merge(pred_df_reg_3,on='fullVisitorId') \n",
    "\n",
    "    final_ensemble_pred['mean'] = final_ensemble_pred.mean(axis=1)\n",
    "\n",
    "    # making zero if mean is negative:\n",
    "    final_ensemble_pred['mean'][final_ensemble_pred['mean'] < 0] = 0\n",
    "    \n",
    "    final_ensemble_pred = final_ensemble_pred.reset_index()\n",
    "    final_ensemble_pred = final_ensemble_pred[['fullVisitorId','mean']]\n",
    "    final_ensemble_pred.columns = ['fullVisitorId', 'PredictedLogRevenue']\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\"prediction for query point done..!\")\n",
    "    \n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # returning the model_predictions:\n",
    "    #----------------------------------------\n",
    "    \n",
    "    \n",
    "    return final_ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean feature preprocessing done..!\n",
      "Numerical feature preprocessing done..!\n",
      "categorical feature preprocessing done..!\n",
      "feature engineering process done..!\n",
      "prediction for query point done..!\n"
     ]
    }
   ],
   "source": [
    "#reading our test data(unzipped file):\n",
    "#--------------------------------------\n",
    "test_df  = pd.read_csv('case study data/preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},index_col=0).reset_index()\n",
    "#--------------------------------------\n",
    "\n",
    "\n",
    "# passing the first 10 rows of test data as a nd-array for predictions:\n",
    "\n",
    "predictions = final_fun_1(test_df.iloc[0:9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullVisitorId</th>\n",
       "      <th>PredictedLogRevenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0459669224143241747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1303090465617023038</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2235365487897339889</td>\n",
       "      <td>0.056965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2866297766347322467</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3461808543879602873</td>\n",
       "      <td>0.029014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>460252456180441002</td>\n",
       "      <td>0.297482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7460955084541987166</td>\n",
       "      <td>0.340299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8381672768065729990</td>\n",
       "      <td>0.067387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>975129477712150630</td>\n",
       "      <td>0.829010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fullVisitorId  PredictedLogRevenue\n",
       "0  0459669224143241747             0.000000\n",
       "1  1303090465617023038             0.000000\n",
       "2  2235365487897339889             0.056965\n",
       "3  2866297766347322467             0.000000\n",
       "4  3461808543879602873             0.029014\n",
       "5   460252456180441002             0.297482\n",
       "6  7460955084541987166             0.340299\n",
       "7  8381672768065729990             0.067387\n",
       "8   975129477712150630             0.829010"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
